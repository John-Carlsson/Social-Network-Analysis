{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Project\n",
    "Written by John Carlsson & Lukas Runt for Social network Analysis spring 2023\n",
    "The tasks are as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Implement, if necessary, optimized versions of the social network mining algorithms seen during the course (diameter, triangles computation, clustering) and test these algorithms on the following datasets:\n",
    "Facebook large\n",
    "High-energy physics theory citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1:\n",
    "import networkx as nw\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import math\n",
    "from scipy.sparse import linalg\n",
    "import random\n",
    "def diameter(G:nw.graph, sample = None):\n",
    "    nodes=G.nodes()\n",
    "    n = len(nodes)\n",
    "    diam = 0\n",
    "    if sample is None:\n",
    "        sample = nodes\n",
    "\n",
    "    for u in sample:\n",
    "        udiam=0\n",
    "        clevel=list()\n",
    "        clevel.append(u)\n",
    "        visited=set()\n",
    "        visited.add(u)\n",
    "        while len(visited) < n:\n",
    "            nlevel=[]\n",
    "            while(len(clevel) > 0):\n",
    "                c=clevel.pop()\n",
    "                for v in G[c]:\n",
    "                    if v not in visited:\n",
    "                        visited.add(v)\n",
    "                        nlevel.append(v)\n",
    "            clevel = nlevel\n",
    "            udiam += 1\n",
    "        if udiam > diam:\n",
    "            diam = udiam\n",
    "    return diam\n",
    "def chunks(data, size):\n",
    "    idata=iter(data)\n",
    "    for i in range(0, len(data), size):\n",
    "        yield {k:data[k] for k in it.islice(idata, size)} \n",
    "def parallel_diam(G,j = cpu_count()):\n",
    "    diam = 0\n",
    "    # Initialize the class Parallel with the number of available process\n",
    "    with Parallel(n_jobs=j) as parallel:\n",
    "        \n",
    "        #Run in parallel diameter function on each processor by passing to each processor only the subset of nodes on which it works\n",
    "        result=parallel(delayed(diameter)(G, X) for X in chunks(G.nodes(), math.ceil(len(G.nodes())/j)))\n",
    "        #Aggregates the results\n",
    "        diam = max(result)\n",
    "    return diam\n",
    "\n",
    "def less(G, edge):\n",
    "    if G.degree(edge[0]) < G.degree(edge[1]):\n",
    "        return 0\n",
    "    if G.degree(edge[0]) == G.degree(edge[1]) and edge[0] < edge[1]:\n",
    "        return 0\n",
    "    return 1\n",
    "def triangles(G:nw.Graph):\n",
    "    num_triangles = 0\n",
    "    m = nw.number_of_edges(G)\n",
    "\n",
    "    # The set of heavy hitters, that is nodes with degree at least sqrt(m)\n",
    "    # Note: the set contains at most sqrt(m) nodes, since num_heavy_hitters*sqrt(m) must be at most the sum of degrees = 2m\n",
    "    # Note: the choice of threshold sqrt(m) is the one that minimize the running time of the algorithm.\n",
    "    # A larger value of the threshold implies a faster processing of triangles containing only heavy hitters, but a slower processing of remaining triangles.\n",
    "    # A smaller value of the threshold implies the reverse.\n",
    "    heavy_hitters=set()\n",
    "    for u in G.nodes():\n",
    "        if G.degree(u) >= math.sqrt(m):\n",
    "            heavy_hitters.add(u)\n",
    "\n",
    "    # Number of triangles among heavy hitters.\n",
    "    # It considers all possible triples of heavy hitters, and it verifies if it forms a triangle.\n",
    "    # The running time is then O(sqrt(m)^3) = m*sqrt(m)\n",
    "    for triple in it.combinations(heavy_hitters,3):\n",
    "        if G.has_edge(triple[0],triple[1]) and G.has_edge(triple[1], triple[2]) and G.has_edge(triple[0], triple[2]):\n",
    "            num_triangles+=1\n",
    "\n",
    "    # Number of remaining triangles.\n",
    "    # For each edge, if one of the endpoints is not an heavy hitter, verifies if there is a node in its neighborhood that forms a triangle with the other endpoint.\n",
    "    # This is essentially the naive algorithm optimized to count only ordered triangle in which the first vertex (i.e., u) is not an heavy hitter.\n",
    "    # Since the size of the neighborhood of a non heavy hitter is at most sqrt(m), the complexity is O(m*sqrt(m))\n",
    "    for edge in G.edges():\n",
    "        sel=less(G,edge)\n",
    "        if edge[sel] not in heavy_hitters:\n",
    "            for u in G[edge[sel]]:\n",
    "                if less(G,[u,edge[1-sel]]) and G.has_edge(u,edge[1-sel]):\n",
    "                    num_triangles +=1\n",
    "\n",
    "    return num_triangles\n",
    "\n",
    "def spectral_clustering(G:nw.Graph):\n",
    "    n=G.number_of_nodes()\n",
    "    nodes = sorted(G.nodes())\n",
    "    # Laplacian of a graph is a matrix, with diagonal entries being the degree of the corresponding node\n",
    "    # and off-diagonal entries being -1 if an edge between the corresponding nodes exists and 0 otherwise\n",
    "    L=nw.laplacian_matrix(G, nodes).asfptype()\n",
    "    # print(L) #To see the laplacian of G uncomment this line\n",
    "    # The following command computes eigenvalues and eigenvectors of the Laplacian matrix.\n",
    "    # Recall that these are scalar numbers w_1, ..., w_k and vectors v_1, ..., v_k such that Lv_i=w_iv_i.\n",
    "    # The first output is the array of eigenvalues in increasing order.\n",
    "    # The second output contains the matrix of eigenvectors:\n",
    "    # specifically, the eigenvector of the k-th eigenvalue is given by the k-th column of v\n",
    "\n",
    "    w, v = linalg.eigsh(L,n-1)\n",
    "    # print(w) #Print the list of eigenvalues\n",
    "    # print(v) #Print the matrix of eigenvectors\n",
    "    # print(v[:,0]) #Print the eigenvector corresponding to the first returned eigenvalue\n",
    "\n",
    "    # Partition in clusters based on the corresponding eigenvector value being positive or negative\n",
    "    # This is known to return (an approximation of) the sparset cut of the graph\n",
    "    # That is, the cut with each of the clusters having many edges, and with few edge among clusters\n",
    "    # Note that this is not the minimum cut (that only requires few edge among clusters,\n",
    "    # but it does not require many edge within clusters)\n",
    "    c1= set()\n",
    "    c2=set()\n",
    "    for i in range(n):\n",
    "\n",
    "        if v[i,0] < 0:\n",
    "            c1.add(nodes[i])\n",
    "        else:\n",
    "            c2.add(nodes[i])\n",
    "    return (c1, c2)\n",
    "\n",
    "def two_means(G):\n",
    "    n=G.number_of_nodes()\n",
    "    # Choose two clusters represented by vertices that are not neighbors\n",
    "    u = random.choice(list(G.nodes()))\n",
    "    v = random.choice(list(nw.non_neighbors(G, u)))\n",
    "    cluster0 = {u}\n",
    "    cluster1 = {v}\n",
    "    added = 2\n",
    "\n",
    "    while added < n:\n",
    "        # Choose a node that is not yet in a cluster and add it to the closest cluster\n",
    "        x = random.choice([el for el in G.nodes() if el not in cluster0|cluster1 and (len(\n",
    "            set(G.neighbors(el)).intersection(cluster0)) != 0 or len(set(G.neighbors(el)).intersection(cluster1)) != 0)])\n",
    "        if len(set(G.neighbors(x)).intersection(cluster0)) != 0:\n",
    "            cluster0.add(x)\n",
    "            added+=1\n",
    "        elif len(set(G.neighbors(x)).intersection(cluster1)) != 0:\n",
    "            cluster1.add(x)\n",
    "            added+=1\n",
    "\n",
    "    return cluster0, cluster1\n",
    "\n",
    "G_fb = nw.Graph()\n",
    "for edge in pd.read_csv('facebook_large/musae_facebook_edges.csv').values:\n",
    "    G_fb.add_edge(*edge) \n",
    "\n",
    "\n",
    "# print(parallel_diam(G_fb)) # Takes around 10 min to run, returns the longest shortest path for the graph. It's 15\n",
    "# print(triangles(G_fb)) # Takes 1 second, return the number of unique triangles in the graph: it's 797516\n",
    "#print(two_means(G_fb)) # Divides the graph into two subgraphs, works by magic, doesnt complete\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Implement the shapley-closeness centrality measure as defined in Michalack et al. (JAIR 2013) sec. 4.4.\n",
    "Implement, if necessary, optimized versions of all studied centrality measures (degree, closeness, betweenness, PageRank, HITS-authority, HITS-hubiness, HITS-both, voterank, shapley-degree, shapley-threshold, shapley-closeness) and test them on the datasets indicated in Task1.\n",
    "The goal of this task is to shortlists the set of centrality measures based on efficiency and similarity of outcomes. Indeed, in the final project you may need to use centrality measures. This task has the goal to shortlist the set of measures that you will use in the final task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "import networkx as nw\n",
    "import math\n",
    "import pandas as pd\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Worker function\n",
    "def worker(subset, G:nw.Graph, num_nodes):\n",
    "    shapley_values = [0] * num_nodes\n",
    "    for i in range(num_nodes):\n",
    "        if i not in subset:\n",
    "            marginal_contribution = (closeness(subset + [i], G) - closeness(subset, G)) / binom(num_nodes - 1, len(subset))\n",
    "            shapley_values[i] += marginal_contribution\n",
    "    return shapley_values\n",
    "\n",
    "# Parallel shapley-wilks centrality function\n",
    "def shapley_closeness_centrality_parallel(G):\n",
    "    num_nodes = len(G.nodes())\n",
    "    shapley_values = [0] * num_nodes\n",
    "\n",
    "    with Pool(processes=cpu_count()) as p:\n",
    "        subsets = [s for s in all_subsets(G.nodes()) if len(s) > 1 and len(s) < num_nodes]\n",
    "        results = p.starmap(worker, [(s, G, num_nodes) for s in subsets])\n",
    "\n",
    "    for result in results:\n",
    "        shapley_values = [x + y for x, y in zip(shapley_values, result)]\n",
    "\n",
    "    return shapley_values\n",
    "\n",
    "\n",
    "# Helper function\n",
    "def all_subsets(S):\n",
    "    n = len(S)\n",
    "    for i in range(2 ** n):\n",
    "        subset = [S[j] for j in range(n) if (i & (1 << j))]\n",
    "    return subset\n",
    "def closeness(S, G):\n",
    "    total_distance = 0\n",
    "    for i in S:\n",
    "        for j in S:\n",
    "            if i < j:\n",
    "                distances = nw.shortest_path(G, i, j)\n",
    "                total_distance += distances[j]\n",
    "    return 1.0 / total_distance\n",
    "def binom(n, k):\n",
    "    return math.factorial(n) / (math.factorial(k) * math.factorial(n-k))\n",
    "    \n",
    "\n",
    "G_fb = nw.Graph()\n",
    "for edge in pd.read_csv('facebook_large/musae_facebook_edges.csv').values:\n",
    "    G_fb.add_edge(*edge) \n",
    "\n",
    "\n",
    "shapley_values = shapley_closeness_centrality_parallel(G_fb)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the VCG, the MUDAN, and the MUDAR, for selling multiple homogeneous items on a social network, with each agent only requiring a single item. The MUDAN and MUDAR algorithm are available on (Fang et al., 2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3755, 4302, 3990]\n"
     ]
    }
   ],
   "source": [
    "# Task 3\n",
    "def auction(k, seller_net, reports, bids, type = 'vcg'):\n",
    "    \"\"\"\n",
    "- k, is the number of items to sell;\n",
    "\n",
    "- seller_net, is a set of strings each identifying a different bidder;\n",
    "\n",
    "- reports, is a dictionary whose keys are strings each identifying a different bidder and whose\n",
    "    values are sets of strings representing the set of bidders to which the bidder identified by the\n",
    "    key reports the information about the auction;\n",
    "\n",
    "- bids, is a dictionary whose keys are strings each identifying a different bidder and whose\n",
    "    values are numbers defining the bid of the bidder identified by that key.\n",
    "\n",
    "- type is the type of auction that is taking place\n",
    "\n",
    "Returns:\n",
    "- allocation, that is a dictionary that has as keys the strings identifying each of the bidders\n",
    "that submitted a bid, and as value a boolean True if this bidder is allocated one of the items,\n",
    "and False otherwise.\n",
    "\n",
    "- payments, that is a dictionary that has as keys the strings identifying each of the bidders that\n",
    "submitted a bid, and as value the price that she pays. Here, a positive price means that the bidder\n",
    "is paying to the seller, while a negative price means that the seller is paying to the bidder.\n",
    "    \"\"\"\n",
    "    allocation, payments = dict(),dict()\n",
    "\n",
    "    for p in bids:\n",
    "        bids[p] = [random.randint(0,1000) for x in range(k)]\n",
    "    \n",
    "    sw = list(map(sum, zip(*[bids[i] for i in bids])))\n",
    "\n",
    "    \n",
    "    #return allocation, payments\n",
    "\n",
    "def vcg():\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "k = 10\n",
    "seller_net = [\"player \"+str(x) for x in range(1,11)]\n",
    "reports = {}\n",
    "bids = {}\n",
    "for s in seller_net:\n",
    "    nei = [random.randint(0,len(seller_net)-1) for i in range(random.randint(0,len(seller_net)-1))]\n",
    "    reports[s] = [seller_net[x] for x in nei if seller_net[x] != s]\n",
    "    bids[s] = []\n",
    "\n",
    "auction(3,seller_net,reports,bids)\n",
    "\n",
    "type = 'vcg'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
