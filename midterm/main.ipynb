{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Project\n",
    "Written by John Carlsson & Lukas Runt for Social network Analysis spring 2023\n",
    "The tasks are as follows:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "Implement, if necessary, optimized versions of the social network mining algorithms seen during the course (diameter, triangles computation, clustering) and test these algorithms on the following datasets:\n",
    "Facebook large\n",
    "High-energy physics theory citation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1:\n",
    "import networkx as nw\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "import pandas as pd\n",
    "import itertools as it\n",
    "import math\n",
    "from scipy.sparse import linalg\n",
    "import random\n",
    "from sklearn.cluster import SpectralClustering\n",
    "import numpy as np\n",
    "\n",
    "def diameter(G:nw.graph, sample = None):\n",
    "    nodes=G.nodes()\n",
    "    n = len(nodes)\n",
    "    diam = 0\n",
    "    if sample is None:\n",
    "        sample = nodes\n",
    "\n",
    "    for u in sample:\n",
    "        udiam=0\n",
    "        clevel=list()\n",
    "        clevel.append(u)\n",
    "        visited=set()\n",
    "        visited.add(u)\n",
    "        while len(visited) < n:\n",
    "            nlevel=[]\n",
    "            while(len(clevel) > 0):\n",
    "                c=clevel.pop()\n",
    "                for v in G[c]:\n",
    "                    if v not in visited:\n",
    "                        visited.add(v)\n",
    "                        nlevel.append(v)\n",
    "            clevel = nlevel\n",
    "            udiam += 1\n",
    "        if udiam > diam:\n",
    "            diam = udiam\n",
    "    return diam\n",
    "\n",
    "def chunks(data, size):\n",
    "    idata=iter(data)\n",
    "    for i in range(0, len(data), size):\n",
    "        yield {k:data[k] for k in it.islice(idata, size)} \n",
    "def parallel_diam(G,j = cpu_count()):\n",
    "    diam = 0\n",
    "    # Initialize the class Parallel with the number of available process\n",
    "    with Parallel(n_jobs=j) as parallel:\n",
    "        \n",
    "        #Run in parallel diameter function on each processor by passing to each processor only the subset of nodes on which it works\n",
    "        result=parallel(delayed(diameter)(G, X) for X in chunks(G.nodes(), math.ceil(len(G.nodes())/j)))\n",
    "        #Aggregates the results\n",
    "        diam = max(result)\n",
    "    return diam\n",
    "\n",
    "def less(G, edge):\n",
    "    if G.degree(edge[0]) < G.degree(edge[1]):\n",
    "        return 0\n",
    "    if G.degree(edge[0]) == G.degree(edge[1]) and edge[0] < edge[1]:\n",
    "        return 0\n",
    "    return 1\n",
    "def triangles(G:nw.Graph):\n",
    "    num_triangles = 0\n",
    "    m = nw.number_of_edges(G)\n",
    "\n",
    "    # The set of heavy hitters, that is nodes with degree at least sqrt(m)\n",
    "    # Note: the set contains at most sqrt(m) nodes, since num_heavy_hitters*sqrt(m) must be at most the sum of degrees = 2m\n",
    "    # Note: the choice of threshold sqrt(m) is the one that minimize the running time of the algorithm.\n",
    "    # A larger value of the threshold implies a faster processing of triangles containing only heavy hitters, but a slower processing of remaining triangles.\n",
    "    # A smaller value of the threshold implies the reverse.\n",
    "    heavy_hitters=set()\n",
    "    for u in G.nodes():\n",
    "        if G.degree(u) >= math.sqrt(m):\n",
    "            heavy_hitters.add(u)\n",
    "\n",
    "    # Number of triangles among heavy hitters.\n",
    "    # It considers all possible triples of heavy hitters, and it verifies if it forms a triangle.\n",
    "    # The running time is then O(sqrt(m)^3) = m*sqrt(m)\n",
    "    for triple in it.combinations(heavy_hitters,3):\n",
    "        if G.has_edge(triple[0],triple[1]) and G.has_edge(triple[1], triple[2]) and G.has_edge(triple[0], triple[2]):\n",
    "            num_triangles+=1\n",
    "\n",
    "    # Number of remaining triangles.\n",
    "    # For each edge, if one of the endpoints is not an heavy hitter, verifies if there is a node in its neighborhood that forms a triangle with the other endpoint.\n",
    "    # This is essentially the naive algorithm optimized to count only ordered triangle in which the first vertex (i.e., u) is not an heavy hitter.\n",
    "    # Since the size of the neighborhood of a non heavy hitter is at most sqrt(m), the complexity is O(m*sqrt(m))\n",
    "    for edge in G.edges():\n",
    "        sel=less(G,edge)\n",
    "        if edge[sel] not in heavy_hitters:\n",
    "            for u in G[edge[sel]]:\n",
    "                if less(G,[u,edge[1-sel]]) and G.has_edge(u,edge[1-sel]):\n",
    "                    num_triangles +=1\n",
    "\n",
    "    return num_triangles\n",
    "\n",
    "def spectral_clustering(G:nw.Graph):\n",
    "    adj_mat = nw.to_numpy_array(G) # Returns an adj_matrix\n",
    "    sc = SpectralClustering(2, affinity='precomputed', n_init=100, assign_labels='discretize')\n",
    "    sc.fit(adj_mat)\n",
    "\n",
    "\n",
    "    return sc.labels_\n",
    "\n",
    "\n",
    "G_fb = nw.Graph()\n",
    "for edge in pd.read_csv('facebook_large/musae_facebook_edges.csv').values:\n",
    "    G_fb.add_edge(*edge) \n",
    "\n",
    "\n",
    "#print(parallel_diam(G_fb)) # Takes around 10 min to run, returns the longest shortest path for the graph. It's 15\n",
    "#print(triangles(G_fb)) # Takes 1 second, return the number of unique triangles in the graph: it's 797516\n",
    "spec_labels = spectral_clustering(G_fb) # Divides the graph into two subgraphs, works by magic, returns labels of the cluster assign to the node, 10 min\n",
    "\n",
    "print(len(spec_labels))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "Implement the shapley-closeness centrality measure as defined in Michalack et al. (JAIR 2013) sec. 4.4.\n",
    "Implement, if necessary, optimized versions of all studied centrality measures (degree, closeness, betweenness, PageRank, HITS-authority, HITS-hubiness, HITS-both, voterank, shapley-degree, shapley-threshold, shapley-closeness) and test them on the datasets indicated in Task1.\n",
    "The goal of this task is to shortlists the set of centrality measures based on efficiency and similarity of outcomes. Indeed, in the final project you may need to use centrality measures. This task has the goal to shortlist the set of measures that you will use in the final task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2\n",
    "import networkx as nw\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def shapley_closeness(G:nw.Graph, f = lambda x: 1/(x+1)):\n",
    "\n",
    "    sv = {i:0 for i in G.nodes()}\n",
    "\n",
    "    for v in G.nodes(): # Parallelise this\n",
    "        print(v)\n",
    "        djk_len = nw.single_source_dijkstra_path_length(G,v) # returns dict with nodes as key and distance as value\n",
    "        djk_len = dict(sorted(djk_len.items(), key=lambda x:x[1])) # Sorted dictionary in ascending order\n",
    "        index_list = list(djk_len.keys().__reversed__()) # sorted in decreasing order and the items are node names\n",
    "        distance = list(djk_len.values().__reversed__())\n",
    "        sum = 0\n",
    "        \n",
    "        prevDistance = -1\n",
    "        prevSV = -1\n",
    "        for index, name in enumerate(index_list, start = 1): # go through the nodes starting with the ine furthest away, index 0\n",
    "            \n",
    "            if distance[index-1] == prevDistance:\n",
    "                currSV = prevSV\n",
    "            else:\n",
    "                currSV = f(distance[index-1])/(1+index) - sum\n",
    "            sv[name] += currSV\n",
    "            sum += f(distance[index-1])/(index*(1+index))\n",
    "            prevDistance = distance[index-1]\n",
    "            prevSV = currSV\n",
    "        sv[v] = f(0) - sum\n",
    "        \n",
    "    return sv\n",
    "\n",
    "G_fb = nw.Graph()\n",
    "for edge in pd.read_csv('facebook_large/musae_facebook_edges.csv').values:\n",
    "    G_fb.add_edge(*edge) \n",
    "\n",
    "\n",
    "shapley_values = shapley_closeness(G_fb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_fb = nw.Graph()\n",
    "import math\n",
    "for edge in pd.read_csv('facebook_large/musae_facebook_edges.csv').values:\n",
    "    G_fb.add_edge(*edge) \n",
    "\n",
    "v = G_fb.number_of_nodes()\n",
    "e = G_fb.number_of_edges()\n",
    "print(v*e+v**2*math.log(v))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the VCG, the MUDAN, and the MUDAR, for selling multiple homogeneous items on a social network, with each agent only requiring a single item. The MUDAN and MUDAR algorithm are available on (Fang et al., 2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3\n",
    "import random\n",
    "from queue import Queue\n",
    "import copy\n",
    "\n",
    "def auction(k, seller_net, reports, bids, type='vcg'):\n",
    "    \"\"\"\n",
    "- k, is the number of items to sell;\n",
    "\n",
    "- seller_net, is a set of strings each identifying a different bidder;\n",
    "\n",
    "- reports, is a dictionary whose keys are strings each identifying a different bidder and whose\n",
    "    values are sets of strings representing the set of bidders to which the bidder identified by the\n",
    "    key reports the information about the auction;\n",
    "\n",
    "- bids, is a dictionary whose keys are strings each identifying a different bidder and whose\n",
    "    values are numbers defining the bid of the bidder identified by that key.\n",
    "\n",
    "- type is the type of auction that is taking place\n",
    "\n",
    "Returns:\n",
    "- allocation, that is a dictionary that has as keys the strings identifying each of the bidders\n",
    "that submitted a bid, and as value a boolean True if this bidder is allocated one of the items,\n",
    "and False otherwise.\n",
    "\n",
    "- payments, that is a dictionary that has as keys the strings identifying each of the bidders that\n",
    "submitted a bid, and as value the price that she pays. Here, a positive price means that the bidder\n",
    "is paying to the seller, while a negative price means that the seller is paying to the bidder.\n",
    "    \"\"\"\n",
    "    payments = {}\n",
    "\n",
    "    items = []\n",
    "    winners = []\n",
    "    for i in range(k):\n",
    "        items.append(random.randint(0, 100))\n",
    "    items.sort(reverse=True)\n",
    "\n",
    "    allocation = {}\n",
    "    for bidder in bids:\n",
    "        allocation[bidder] = False\n",
    "        payments[bidder] = 0\n",
    "\n",
    "    for p in bids:\n",
    "        bids[p] = [random.randint(0, 100) for x in range(k)]\n",
    "\n",
    "    #allocation\n",
    "    for i in range(k):\n",
    "        sorted_bidders = sorted(bids.items(), key=lambda x:x[1][i], reverse=True)\n",
    "        j = 0\n",
    "        while allocation[sorted_bidders[j][0]]:\n",
    "            j += 1\n",
    "        winners.append(sorted_bidders[j][0])\n",
    "        allocation[sorted_bidders[j][0]] = True\n",
    "\n",
    "    #bfs - measuring distance\n",
    "    start = random.randint(0, len(seller_net) - 1)\n",
    "    auction = 0\n",
    "    for winner in winners:\n",
    "        distance = {}\n",
    "        for seller in seller_net:\n",
    "            distance[seller] = -1\n",
    "        queue = Queue()\n",
    "        queue.put(seller_net[start])\n",
    "        distance[seller_net[start]] = 0\n",
    "        while queue:\n",
    "            vertex = queue.get()\n",
    "            if vertex == winner:\n",
    "                distance[winner] = distance[vertex] + 1\n",
    "                payments[winner] = bids[winner][auction] - distance[winner]\n",
    "                print(distance)\n",
    "                break\n",
    "            link = reports[vertex]\n",
    "            for l in link:\n",
    "                if distance[l] == -1:\n",
    "                    distance[l] = distance[vertex] + 1\n",
    "                    queue.put(l)\n",
    "        auction += 1\n",
    "\n",
    "    #computing social welfare\n",
    "    social_welfare = 0\n",
    "    for i in range(k):\n",
    "        social_welfare += items[i] * bids[winners[i]][i]\n",
    "\n",
    "    print(social_welfare)\n",
    "\n",
    "    print(allocation)\n",
    "    print(payments)\n",
    "    return allocation, payments\n",
    "\n",
    "def allocater(k, bid:dict):\n",
    "    # initialization, one bidder can only win one item\n",
    "    allocation = {bidder:False for bidder in bid}\n",
    "    bids = copy.copy(bid)\n",
    "    for bidder in bids:\n",
    "        allocation[bidder] = False\n",
    "\n",
    "def data_init():\n",
    "    seller_net = []\n",
    "    reports = {}\n",
    "    file = open(\"musae_facebook_edges.csv\", \"r\")\n",
    "    file.readline()\n",
    "    for line in file:\n",
    "        modified = line.strip(\"\\n\")\n",
    "        splited = modified.split(\",\")\n",
    "        if not splited[0] in seller_net:\n",
    "            seller_net.append(splited[0])\n",
    "            reports[splited[0]] = []\n",
    "        reports[splited[0]].append(splited[1])\n",
    "    file.close()\n",
    "    print(seller_net)\n",
    "    print(reports)\n",
    "    return seller_net, reports\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #data_init()\n",
    "    k = 10\n",
    "    seller_net = [\"player \" + str(x) for x in range(1, 11)]\n",
    "    # keys inentifying bidder,\n",
    "    reports = {}\n",
    "    bids = {}\n",
    "    for s in seller_net:\n",
    "        nei = [random.randint(0, len(seller_net) - 1) for i in range(random.randint(0, len(seller_net) - 1))]\n",
    "        reports[s] = [seller_net[x] for x in nei if seller_net[x] != s]\n",
    "        bids[s] = []\n",
    "\n",
    "    print(seller_net)\n",
    "    print(reports)\n",
    "    #print(bids)\n",
    "\n",
    "    #print(allocation(k, bids))\n",
    "    auction(3, seller_net, reports, bids)\n",
    "\n",
    "    type = 'vcg'\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement an environment for bandits algorithm that will model the following setting:\n",
    "We are given a social network such that with a probability p(u,v) for each edge (u,v). These probabilities are unknown to the learner. The learner at each time steps interacts with this environment by choosing a vertex x. The environment set each edge (u,v) as alive with probability p(u,v), and dead otherwise. It then assigns as a reward to the learner that is equivalent to the number of nodes of the social network that are reachable from the selected vertex x through alive edges only.\n",
    "Given that the goal of the auctioneer is to maximize her cumulative reward, find the best bandit algorithm (either found among the ones implemented during the course, or a completely new designed algorithm) that the auctioneer may use in above setting (i.e., the algorithm must decide which vertex to select at each time step).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nw\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bernoulli\n",
    "\n",
    "class Environment:\n",
    "    #We initialise the environment, keep or remove the edges (dead or alive) based on the prbability\n",
    "    def __init__(self, g:nw.Graph, p):\n",
    "        self.__graph = g\n",
    "        self.__prob = p # The probability that an edge is alive\n",
    "\n",
    "    #It select the reward for the given arm according to the distribution with the given mean\n",
    "    def receive_reward(self, arm):\n",
    "        \n",
    "        edge_list = list()\n",
    "\n",
    "        for edge in list(self.__graph.edges()):\n",
    "            if not bernoulli.rvs(self.__prob[edge]): # if dead\n",
    "                edge_list.append(edge)\n",
    "                self.__graph.remove_edge(*edge)\n",
    "\n",
    "        reward = len(nw.descendants(self.__graph,arm))\n",
    "\n",
    "        for edge in edge_list:\n",
    "            self.__graph.add_edge(*edge)\n",
    "        \n",
    "\n",
    "        return reward\n",
    "\n",
    "\n",
    "class UCB_Learner:\n",
    "\n",
    "    def __init__(self, arms_set, T, environment:Environment): #USE THIS FOR KNOWN TIME HORIZON\n",
    "    #def __init__(self, arms_set, environment): #USE THIS FOR UNKNOWN TIME HORIZON\n",
    "        self.__arms_set = arms_set #initialize the set of arms\n",
    "        self.__environment = environment #initialize the environment\n",
    "        self.__T = T #COMMENT THIS FOR UNKNOWN TIME HORIZON\n",
    "        #self.__t = 1 #UNCOMMENT THIS FOR UNKNOWN TIME HORIZON\n",
    "        # Next initialization will serve for computing the best strategy during exploitation steps\n",
    "        self.__num = {a:0 for a in arms_set} #It saves the number of times arm a has been selected\n",
    "        self.__rew = {a:0 for a in arms_set} #It saves the cumulative reward achieved by arm a when selected\n",
    "        #It saves the ucb value of each arm until the current time step\n",
    "        #It is initialised to infinity in order to allow that each arm is selected at least once\n",
    "        self.__ucb = {a:float('inf') for a in arms_set}\n",
    "\n",
    "    # This function returns the arm chosen by the learner and the corresponding reward returned by the environment\n",
    "    def play_arm(self):\n",
    "        a_t = max(self.__ucb, key=self.__ucb.get)  #We choose the arm that has the highest average revenue\n",
    "\n",
    "        reward = self.__environment.receive_reward(a_t) #We save the reward assigned by the environment\n",
    "        # We update the number of times arm a_t has been chosen, its cumulative reward and its UCB value\n",
    "        self.__num[a_t] += 1\n",
    "        self.__rew[a_t] += reward\n",
    "        #COMMENT THE FOLLOWING LINE FOR UNKNOWN TIME HORIZON\n",
    "        self.__ucb[a_t] = self.__rew[a_t]/self.__num[a_t] + math.sqrt(2*math.log(self.__T)/self.__num[a_t])\n",
    "        #UNCOMMENT THE FOLLOWING LINES FOR UNKNOWN TIME HORIZON\n",
    "        #self.__ucb[a_t] = self.__rew[a_t] / self.__num[a_t] + math.sqrt(2 * math.log(self.__t) / self.__num[a_t])\n",
    "        #self.__t += 1\n",
    "\n",
    "        return a_t, reward\n",
    "\n",
    "\n",
    "class EpsGreedy_Learner:\n",
    "    def __init__(self, arms_set, T, environment:Environment, eps): #USE THIS FOR KNOWN TIME HORIZON\n",
    "    #def __init__(self, arms_set, environment, eps): #USE THIS FOR UNKNOWN TIME HORIZON\n",
    "        self.__arms_set = list(arms_set) #initialize the set of arms\n",
    "        self.__environment = environment #initialize the environment\n",
    "        self.__T = T #COMMENT THIS FOR UNKNOWN TIME HORIZON\n",
    "        self.__eps = eps #initialize the sequence of eps_t\n",
    "        #Next initialization will serve for computing the best strategy during exploitation steps\n",
    "        self.__num = {a:0 for a in arms_set} #It saves the number of times arm a has been selected\n",
    "        self.__rew = {a:0 for a in arms_set} #It saves the cumulative reward achieved by arm a when selected\n",
    "        self.__avgrew = {a:0 for a in arms_set}  #It saves the average reward achieved by arm a until the current time step\n",
    "        self.__t = 0 #It saves the current time step\n",
    "\n",
    "    #This function returns the arm chosen by the learner and the corresponding reward returned by the environment\n",
    "    def play_arm(self):\n",
    "        a_t = max(self.__avgrew, key=self.__avgrew.get) #We choose the arm that has the highest average revenue\n",
    "        \n",
    "        r = random.random()\n",
    "        if r <= self.__eps[self.__t]: #With probability eps_t\n",
    "        #if r <= self.__eps(self.__t): USE THIS FOR UNKNOWN TIME HORIZON\n",
    "            a_t = random.choice(self.__arms_set) #We choose an arm uniformly at random\n",
    "\n",
    "        reward = self.__environment.receive_reward(a_t) #We save the reward assigned by the environment\n",
    "        #We update the number of times arm a_t has been chosen, its cumulative and its average reward\n",
    "        self.__num[a_t] += 1\n",
    "        self.__rew[a_t] += reward\n",
    "        self.__avgrew[a_t] = self.__rew[a_t] / self.__num[a_t]\n",
    "        self.__t += 1 #We are ready for a new time step\n",
    "\n",
    "        return a_t, reward\n",
    "    \n",
    "# If we want to make our own learner\n",
    "class CB_Learner:\n",
    "    def __init__(self, arms_set, T, environment:Environment): #USE THIS FOR KNOWN TIME HORIZON\n",
    "    #def __init__(self, arms_set, environment): #USE THIS FOR UNKNOWN TIME HORIZON\n",
    "        self.__arms_set = list(arms_set) #initialize the set of arms\n",
    "        self.__arms_iter = iter(self.__arms_set)\n",
    "        self.__environment = environment #initialize the environment\n",
    "        self.__T = T #COMMENT THIS FOR UNKNOWN TIME HORIZON\n",
    "        self.__t = 1 # count the time\n",
    "        # Next initialization will serve for computing the best strategy during exploitation steps\n",
    "        self.__num = {a:0 for a in arms_set} #It saves the number of times arm a has been selected\n",
    "        self.__rew = {a:0 for a in arms_set} #It saves the cumulative reward achieved by arm a when selected\n",
    "        #It saves the ucb value of each arm until the current time step\n",
    "        #It is initialised to infinity in order to allow that each arm is selected at least once\n",
    "        self.__ucb = {a:float('inf') for a in arms_set}\n",
    "        self.__lcb = {a:-float('inf') for a in arms_set}\n",
    "\n",
    "    # This function returns the arm chosen by the learner and the corresponding reward returned by the environment\n",
    "    def play_arm(self):\n",
    "        a_t = self.__arms_set[self.__t-1]  #We choose the first arm\n",
    "\n",
    "        reward = self.__environment.receive_reward(a_t) #We save the reward assigned by the environment\n",
    "        # We update the number of times arm a_t has been chosen, its cumulative reward and its UCB value\n",
    "        self.__num[a_t] += 1\n",
    "        self.__rew[a_t] += reward\n",
    "        #COMMENT THE FOLLOWING LINE FOR UNKNOWN TIME HORIZON\n",
    "        self.__ucb[a_t] = self.__rew[a_t]/self.__num[a_t] + math.sqrt(2*math.log(self.__T)/self.__num[a_t])\n",
    "        self.__lcb[a_t] = self.__rew[a_t]/self.__num[a_t] - math.sqrt(2*math.log(self.__T)/self.__num[a_t])\n",
    "        #UNCOMMENT THE FOLLOWING LINES FOR UNKNOWN TIME HORIZON\n",
    "        #self.__ucb[a_t] = self.__rew[a_t] / self.__num[a_t] + math.sqrt(2 * math.log(self.__t) / self.__num[a_t])\n",
    "        self.__t += 1\n",
    "        if self.__t == len(self.__arms_set):\n",
    "            self.deactivate_arms()\n",
    "            self.__t = 1\n",
    "\n",
    "        return a_t, reward\n",
    "    \n",
    "    def deactivate_arms(self):\n",
    "        a = max(self.__lcb, key = self.__lcb.get)\n",
    "        for arm in self.__arms_set:\n",
    "            if arm != a: \n",
    "                if self.__lcb[a] > self.__ucb[arm]:\n",
    "                    self.__arms_set.remove(arm)\n",
    "          \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    #INITIALIZATION\n",
    "    # Generate a graph\n",
    "    \n",
    "    g = nw.gnp_random_graph(1000,0.001,directed=True)\n",
    "\n",
    "    #Optimal Arm in Hindsight: I need it for computing the regret\n",
    "    d = {i:len(nw.descendants(g,i)) for i in g.nodes()}\n",
    "    opt_a = max(d,key = d.get) # With all edges alive, which is the best node\n",
    "\n",
    "    p = {i:random.random() for i in g.edges()}\n",
    "    #Time Horizon\n",
    "    T = 500\n",
    "    #We would like to evaluate the expected regret with respect to t\n",
    "    #To this aim, we cannot just run a single simulation:\n",
    "    #the result can be biased by extreme random choices (of both the environment and the learner)\n",
    "    #For this reason we run N simulations,\n",
    "    #and we will evaluate against t the average regret over the N simulations\n",
    "    #To this aim, we define N, and we will record for each learner a matrix containing\n",
    "    #the regret for each simulation and each time step within the simulation\n",
    "    N = 10 #number of simulations\n",
    "    eps = [1] #for the first step we cannot make exploitation, so eps_1 = 1\n",
    "    eps.extend((len(g.nodes())*math.log(t)/t)**(1/3) for t in range(2,T+1))\n",
    "    ucb_regrets = {n: {t: 0 for t in range(T)} for n in range(N)} #regret matrix for the UCB learner\n",
    "    eps_regrets = {n: {t: 0 for t in range(T)} for n in range(N)} #regret matrix for the eps learner\n",
    "    cb_regrets = {n: {t: 0 for t in range(T)} for n in range(N)} #regret matrix for the CB learner\n",
    "    \n",
    "    \n",
    "\n",
    "   #SIMULATION PLAY\n",
    "    for n in range(N):\n",
    "        print(n)\n",
    "        ucb_cum_reward = 0 #it saves the cumulative reward of the UCB learner\n",
    "        eps_cum_reward = 0 #it saves the cumulative reward of the eps-greedy learner\n",
    "        cum_opt_reward = 0 #it saves the cumulative reward of the best-arm in hindsight\n",
    "        cb_cum_reward = 0 \n",
    "        \n",
    "        #Environment\n",
    "        env = Environment(g,p)\n",
    "        #Eps-Greedy Learner\n",
    "        eps_learn = EpsGreedy_Learner(g.nodes(), T, env, eps) #COMMENT FOR UNKNOWN TIME HORIZON\n",
    "        #eps_learn = EpsGreedy_Learner(arms_set, env, eps) #UNCOMMENT FOR UNKNOWN TIME HORIZON\n",
    "        #UCB Learner\n",
    "        ucb_learn = UCB_Learner(g.nodes(), T, env)  #COMMENT FOR UNKNOWN TIME HORIZON\n",
    "        #ucb_learn = UCB_Learner(arms_set, env)  #UNCOMMENT FOR UNKNOWN TIME HORIZON\n",
    "\n",
    "        cb_learn = CB_Learner(g.nodes(), T, env)\n",
    "\n",
    "        for t in range(T):\n",
    "\n",
    "            #reward obtained by the optimal arm\n",
    "            cum_opt_reward += env.receive_reward(opt_a)\n",
    "            \n",
    "            # reward obtained by the ucb learner\n",
    "\n",
    "            a, reward = ucb_learn.play_arm()\n",
    "            ucb_cum_reward += reward\n",
    "            #regret of the ucb learner\n",
    "            ucb_regrets[n][t] = cum_opt_reward - ucb_cum_reward\n",
    "            #reward obtained by the eps_greedy learner\n",
    "\n",
    "            a, reward = eps_learn.play_arm()\n",
    "            eps_cum_reward += reward\n",
    "            #regret of the eps_greedy learner\n",
    "            eps_regrets[n][t] = cum_opt_reward - eps_cum_reward\n",
    "\n",
    "            # reward obtained by the CB learner\n",
    "            a, reward = cb_learn.play_arm()\n",
    "            cb_cum_reward += reward\n",
    "            #regret of the ucb learner\n",
    "            cb_regrets[n][t] = cum_opt_reward - cb_cum_reward\n",
    "\n",
    "    #compute the mean regret of the eps greedy and ucb learner\n",
    "    eps_mean_regrets = {t:0 for t in range(T)}\n",
    "    ucb_mean_regrets = {t:0 for t in range(T)}\n",
    "    cb_mean_regrets = {t:0 for t in range(T)}\n",
    "    for t in range(T):\n",
    "        eps_mean_regrets[t] = sum(eps_regrets[n][t] for n in range(N))/N\n",
    "        ucb_mean_regrets[t] = sum(ucb_regrets[n][t] for n in range(N))/N\n",
    "        cb_mean_regrets[t] = sum(cb_regrets[n][t] for n in range(N))/N\n",
    "\n",
    "    #VISUALIZATION OF RESULTS\n",
    "    #compute t^2/3 (c K log t)^1/3\n",
    "    ref_eps = list()\n",
    "    for t in range(1, T+1):\n",
    "        ref_eps.append((t**(2/3))*(2*len(g.nodes())*math.log(t))**(1/3))\n",
    "\n",
    "    #compute c*sqrt(KtlogT)\n",
    "    ref_ucb = list()\n",
    "    for t in range(1, T+1):\n",
    "        ref_ucb.append(math.sqrt(len(g.nodes())*t*math.log(T)))\n",
    "\n",
    "    #compute c*sqrt(KtlogT)\n",
    "    ref_cb = list()\n",
    "    for t in range(1, T+1):\n",
    "        ref_cb.append(math.sqrt(len(g.nodes())*t*math.log(T)))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    fig, (ax1,ax2,ax3,ax4) = plt.subplots(4)\n",
    "    #Plot eps-greedy regret against its reference value\n",
    "    ax1.plot(range(1,T+1), eps_mean_regrets.values(), label = 'eps_mean_regret')\n",
    "    ax1.plot(range(1,T+1), ref_eps, label = f't^2/3 (2 K log t)^1/3')\n",
    "    ax1.set_xlabel('t')\n",
    "    ax1.set_ylabel('E[R(t)]')\n",
    "    ax1.legend()\n",
    "\n",
    "    #Plot ucb regret against its reference value\n",
    "    ax2.plot(range(1,T+1), ucb_mean_regrets.values(), label = 'ucb_mean_regret')\n",
    "    ax2.plot(range(1,T+1), cb_mean_regrets.values(), label = 'CB_mean_regret')\n",
    "    #ax2.plot(range(1,T+1), ref_ucb, label = f'sqrt(K*t*logT)')\n",
    "    ax2.set_xlabel('t')\n",
    "    ax2.set_ylabel('E[R(t)]')\n",
    "    ax2.legend()\n",
    "\n",
    "    #Plot ucb regret against its reference value\n",
    "    ax3.plot(range(1,T+1), cb_mean_regrets.values(), label = 'CB_mean_regret')\n",
    "    #ax3.plot(range(1,T+1), ref_cb, label = f'sqrt(K*t*logT)')\n",
    "    ax3.set_xlabel('t')\n",
    "    ax3.set_ylabel('E[R(t)]')\n",
    "    ax3.legend()\n",
    "\n",
    "    #Plot ucb regret against eps-greedy regret\n",
    "    ax4.plot(range(1,T+1), eps_mean_regrets.values(), label = 'eps_mean_regret')\n",
    "    ax4.plot(range(1,T+1), ucb_mean_regrets.values(), label = 'ucb_mean_regret')\n",
    "    ax4.plot(range(1,T+1), cb_mean_regrets.values(), label = 'cb_mean_regret')\n",
    "    ax4.set_xlabel('t')\n",
    "    ax4.set_ylabel('E[R(t)]')\n",
    "    ax4.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
